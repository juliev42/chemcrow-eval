# chemcrow-eval
Exploring methods to evaluate scientific reasoing, for e.g. chemcrow and similar agents. 

## Why evaluating scientific reasoning tasks with LLMs is hard
As described in 'Augmenting large language models with chemistry tools' (Bran & Cox et al. 2023), 
GPT-4 frequently cannot easily distinguish between right and wrong answers in scientific tasks. It tends to 
rate answers written by the base GPT-4 model higher than answers that incorporate additional tools for scientific accuracy. This 
may reflect a key limitation of using evaluation LLMs to assess other LLMs: the evaluating LLM may rank answers higher that are 
more likely to be generated by the evaluating LLM. This perpetuates limitations of the evaluating LLMs. 

In order to better assess scientific reasoning tasks, we may explore ways to create better evaluation LLMs.
We might try training or fine-tuning LLMs on scientific literature if we have the resources, or use
prompt engineering and retrieval-augmented generation to improve the base models for evaluation purposes.
We could also consider using some of the same tools that are present model being evaluated. This repo will 
compare a few MRKL (Modular Reasoning, Knowledge & Language) models in evaluation tasks. 


## A Few Evaluation Approaches

Baseline: 

0. Prompt to think like a chemistry professor grading an important exam

Multi-Stage Reasoning Without RAG

0. Prompt to think like a chemistry professor grading an important exam
1. Break down the response into steps, further break down into assumed facts
2. Evaluate each fact in each individual step for correctness and coherenece
3. Evaluate the entirety of the task for correctness 

Multi-Stage Reasoning With RAG

0. Prompt to think like a chemistry professor grading an important exam
1. Break down the response into steps, further break down into assumed facts
2. Retrieve relevant reference information for each step using tools from a database, e.g. Google search or a scientific database
3. Evaluate each fact in each individual step for correctness using the reference materials
4. Evaluate the entirety of the task for correctness 

Future: Few-Shot Learning with Multi-Stage Reasoning

Similar to above, but prompt with a few expert-annotated examples. 

